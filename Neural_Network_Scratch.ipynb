{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Prashant Shinde\n",
    "# Reference: https://www.youtube.com/watch?v=kft1AJ9WVDk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Build a Neural Network (Forward + Back prop) using NumPy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "''' Perceptron: A Neural Network with no hidden layer \n",
    " Perceptron: Input layer -> Neuron -> Output layer\n",
    " Synapses: Connections between Input & Neuron (synapses has WEIGHTS)\n",
    " WEIGHTS: Strong positive or negative numbers that affects neurons output \n",
    " Initialise weights of synapses with 'random' numbers\n",
    " At Neuron: Normalize(weighted sum of inputs) = Normalize(SUM(input * weights))\n",
    " Normalizing (non linear) function: ouput between 0 & 1 (e.g. sigmoid, ReLU, tanh)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Forward Propogation\n"
     ]
    }
   ],
   "source": [
    "print('A: Forward Propogation')\n",
    "# Sequence: input -> (optimize) synapses -> neuron -> output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Normalizing function: Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input:\n",
      " [[0 0 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "Training Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Input data (4x3 matrics): 4 examples & 3 inputs\n",
    "training_inputs = np.array([[0,0,1],\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]\n",
    "                           ])\n",
    "\n",
    "# Output data (4x1 matrics): 4 examples & 1 output\n",
    "training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "print('Training Input:\\n', training_inputs)\n",
    "print('Training Output: \\n', training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights:\n",
      " [[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Mean of synaptic weights: -0.24169275135861534\n"
     ]
    }
   ],
   "source": [
    "# Initialise weights\n",
    "np.random.seed(1)\n",
    "\n",
    "# 3 weights (w1, w2, w3) becasue we have 3 inputs / synapses\n",
    "# Use random values from -1 to +1 with mean = 0\n",
    "synaptic_weights = 2 * np.random.random((3,1)) - 1\n",
    "print('Random starting synaptic weights:\\n', synaptic_weights)    \n",
    "print('Mean of synaptic weights:', synaptic_weights.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training: \n",
      " [[0.2689864 ]\n",
      " [0.3262757 ]\n",
      " [0.23762817]\n",
      " [0.36375058]]\n",
      "BUT Expected output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#Forward PATH only (Terrible prediction results)\n",
    "for iteration in range(1):\n",
    "    \n",
    "    input_layer = training_inputs\n",
    "    \n",
    "    weights = synaptic_weights\n",
    "    \n",
    "    outputs = sigmoid(np.dot(input_layer, weights))\n",
    "    \n",
    "print('Output after training: \\n', outputs)\n",
    "print('BUT Expected output: \\n', training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: Backward Propogation\n"
     ]
    }
   ],
   "source": [
    "print('B: Backward Propogation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have to build a BACKPROPOGATION path to update weights based on PREDICTION ERROR\n",
      "Backpropogation key: Error weighted derivatives\n",
      "Adjust weights by = error * input * (Gradient of sigmoid function at predicted output)\n",
      "Weight adjustment is proportional to size of the error\n"
     ]
    }
   ],
   "source": [
    "print('We have to build a BACKPROPOGATION path to update weights based on PREDICTION ERROR')\n",
    "print('Backpropogation key: Error weighted derivatives')\n",
    "\n",
    "# To get CORRECT OUTPUT (/learn):\n",
    "# Get initial predictions using randomly initialised weights: DONE\n",
    "# Calculate Error =  Actual - Predicted output\n",
    "# Derivatives: Adjust weights based on severeness of the error: Error weighted derivatives\n",
    "# Repeat this process thousands of time\n",
    "\n",
    "print('Adjust weights by = error * input * (Gradient of sigmoid function at predicted output)')\n",
    "print('Weight adjustment is proportional to size of the error')\n",
    "\n",
    "# If output if very large or very small meaning neuron is confident (confident neuron = no adjustment needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To adjust weights in backpropogation (Error weighted derivatives)\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synaptic_weights After training: \n",
      " [[10.38040701]\n",
      " [-0.20641179]\n",
      " [-4.98452047]]\n",
      "Output after training: \n",
      " [[0.00679672]\n",
      " [0.99445583]\n",
      " [0.99548516]\n",
      " [0.00553614]]\n",
      "AND Expected output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(20000):\n",
    "    \n",
    "    # Forward propogation starts ...\n",
    "    input_layer = training_inputs\n",
    "    \n",
    "    outputs = sigmoid(np.dot(input_layer, synaptic_weights))\n",
    "    #print('Output is:\\n', outputs)\n",
    "    #print('training_outputs is: \\n', training_outputs)\n",
    "    \n",
    "    # Backpropogation starts ... \n",
    "    # calculate error\n",
    "    error = training_outputs - outputs\n",
    "    #print('error is:\\n', error)\n",
    "    \n",
    "    # calculate adjustment based on error (high, low)\n",
    "    adjustments = error * sigmoid_derivative(outputs)\n",
    "    #print('adjustments are: \\n', adjustments)\n",
    "    \n",
    "    # update weights\n",
    "    synaptic_weights += np.dot(input_layer.T, adjustments)\n",
    "    # synaptic_weights = synaptic_weights + np.dot(input_layer.T, adjustments)\n",
    "\n",
    "\n",
    "print('synaptic_weights After training: \\n', synaptic_weights)\n",
    "print('Output after training: \\n', outputs)\n",
    "print('AND Expected output: \\n', training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More about Backpropogation (backprop) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Paper: Learning representations by backpropogating errors\n",
    "- Backpropogation uses partial derivatives & chain rule \n",
    "- Repeatedly adjust weights of the connections in the network so as to minimize a measure of the difference between actual output vector of the net & predicted output vector\n",
    " - Activation: is output of a neuron after applying an activation function\n",
    " - Forward Propagation is applied each time you predict a Y given X. Back propagation is applied only at training on each epoch in order to update the weights (along with forward propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More about Activation Functions\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
